---
title: 上海疫情数据简单可视化分析(4月11日更新)
author: Luo Fei
date: '2022-03-24'
slug: covidsh
categories:
  - 中文
tags:
  - Tec
  - R
output:
    blogdown::html_page:
        number_sections: true
math: true
showToc: true
TocOpen: false
toc-title: "目录"
draft: false
hidemeta: false
comments: false
description: "无责任猜测。"
canonicalURL: ""
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: true
ShowPostNavLinks: true
cover:
    image: "" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page    
---

```{r,setup,include=FALSE}
options(digits = 2)

##knitr设置
knitr::opts_chunk$set(
  prompt = FALSE,
  comment = "#",
  results = "markup",
  error = FALSE,
  message = FALSE,
  fig.align = "center",
  collapse = FALSE,
  tidy = F,
  echo = F,
  message = F,
  results = "hide",
  warning = F,
  cache.path = "cash/",
  cache = T
  
  )

```
今天本地的疫情终于没有增加，抽了点时间关注其他地区的疫情形势。看官方通报的数据，对曾经的模范城市的疫情有兴趣，决定来简单看看。

# 数据获取

## 数据来源

要获取准确的数据，当然是上官方网站。打开上海市卫健委的官网(https://wsjkw.sh.gov.cn/xwfb/index.html)，疫情数据公告都在“新闻发布”栏目中疫情信息的标题中就包含了所有新增、确诊数据。真是太方便了。



```{r, eval=TRUE, include=TRUE, warning=FALSE, echo=TRUE}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)
library(openxlsx)
library(ggforce)
library(mgcv)
library(deSolve)
library(FME)
```






```{r, eval=FALSE, echo=FALSE}
## 设置url
url <- "https://wsjkw.sh.gov.cn/xwfb/index.html"
## 获取首页信息
content <- read_html(url)
reportdate <- content %>%  html_nodes(".time") %>% html_text() %>% as.Date.character()
title <-  content %>% html_nodes(".list-date a") %>% html_text() 
basedata <- tibble(reportdate = reportdate, title = title)

```

## 数据

上海的疫情变化主要从3月开始

```{r, eval=FALSE, echo=FALSE}
##使用循环爬取2到200也网页信息
url_1 <- c("https://wsjkw.sh.gov.cn/xwfb/index.html",
           paste0("https://wsjkw.sh.gov.cn/xwfb/index_", c(2:200), ".html"))
webfun_0 <- function(url){
    webpage <- read_html(url) 
    tibble( "reportdate" = webpage %>% html_nodes(".time") %>% html_text(),
    "title" = webpage %>% html_nodes(".list-date a") %>% html_attr("title"))
  }
basedata <- map_df(url_1, webfun_0) 


## 写出数据
write.csv(basedata,"data/basedata.csv")

```

## 数据清洗

这一步比较麻烦的是对标题中日期字符的整理。使用`str_extract_all`命令后提取的日期，变成了列表。再合并为向量形势的日期格式数据时出了点麻烦。最后使用了笨办法`for`循环`unlist`后再`paste0`合并。其实直接用标题前的日期-1没什么大的误差，主要是在跟自己较劲搞得这么麻烦。

```{r, include=FALSE, message=FALSE}
basedata <- read.csv("data/basedata.csv")
## 筛选出包含疫情信息的标题
sh01 <-  basedata %>% filter(str_detect(title, "新冠肺炎确诊病例"))

sh02 <- sh01 %>% mutate(
  reportdate = as.Date.character(reportdate, "%Y-%m-%d"),
  title = if_else(str_detect(title, "本地"), str_replace(title, "本地", "本土"), title),
  检出阳性日期 = str_extract(title, "[0-9]+年[0-9]+月[0-9]+日") %>% as.Date.character("%Y年%m月%d日"),
  新增确诊 = str_extract(title, "新增本土新冠肺炎确诊病例[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  新增无症状 = str_extract(title, "无症状感染者[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  新增境外输入确诊 = str_extract(title, "境外.+[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  新增境外无症状输入 = str_extract(title, "境外输入性无症状感染者+[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  治愈数 = str_extract(title, "治愈出院[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  解除 = str_extract(title, "解除.+") %>% str_extract("[0-9]+") %>% as.numeric())



sh02 <- sh02 %>% mutate(
  检出阳性日期 = if_else(is.na(检出阳性日期), reportdate-1, 检出阳性日期)
)
sh02[is.na(sh02)] <- 0
sh02 <- sh02 %>% mutate(
  新增本地感染 = 新增确诊 + 新增无症状,
  新增境外输入感染 = 新增境外输入确诊 + 新增境外无症状输入
 )
## 转换为长数据

sh_long <-  sh02 %>% pivot_longer(cols =  新增确诊:新增境外输入感染, names_to = "感染者类型", values_to = "病例数" )



```

# 好了，开始分析吧

## 先画个简单的图看看大趋势

以报告感染者类型为颜色看，从20年1月到21年12月期间，上海的感染人数几乎在处于一个长期稳定的状态，从2022年3月开始呈直升飞机式的增长。奇怪的是中间咋有个空白区，没有数据。查看原始网页，发现网站从2021年11月6日-2022年1月1日没有更新数据。这是一个奇怪的现象。不过没关系，这不影响我们后面的分析，~~这对原因分析有很大关系~~。

```{r, results='asis'}

sh_long %>% ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(alpha = 0.3, position = "jitter") + 
  scale_x_date(date_breaks = "3 month", date_labels = "%y-%m")
```

##  输入感染者的趋势


对上海这波疫情有个合理的猜测是，1月初上海接纳大量某地的航班的，导致上海市输入疫情压力陡增，再加上Omicron变异株超强传播能力，双重压力下导致这个模范城市失守。好吧，我们来看看是否能验证输入压力陡增这个猜测。从图上看新增输入的感染者数量变化并不大，鉴于确诊和无症状都属与感染这，下一步我们把本地和输入的感染者的合计数的变化可视化看看。

```{r, results='asis'}

sh_long %>% filter( 感染者类型 == "新增境外输入感染"  ) %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(position = "jitter", alpha = 0.2) + 
  scale_x_date(date_breaks = "3 month", date_labels = "%y-%m")

```

## 感染者总数

先用二者做个散点图看看,如图，完全看不出啥关系啊。。。。输入感染者较多的时候，反而本地感染处于低水平。这个图像，线性回归暂时也不考虑。我们还是从感染者数量和时间的关系看看。



```{r, results='asis'}
sh_long %>% filter(检出阳性日期 > as.Date("2022-2-11")) %>% 
  filter(感染者类型 == "新增境外输入感染"|感染者类型 == "新增本地感染") %>% 
  select(检出阳性日期, 感染者类型, 病例数) %>% pivot_wider(names_from = 感染者类型, values_from = 病例数) %>% 
  ggplot() +
  geom_point(aes(x = 新增本地感染, y = 新增境外输入感染))
```

画出来如下，大致能看出在3月14日前，上海几乎没有本土感染病例。
```{r, results='asis'}
sh_long %>% filter(感染者类型 == "新增境外输入感染"|感染者类型 == "新增本地感染")  %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(alpha = 0.5) + 
  geom_line(alpha = 0.5) +
  scale_x_date(date_breaks = "3 month", date_labels = "%y-%m") 

```

## 2022年1到三月


我们把时间尺度拉大到2月中旬到3月看看。放大后（下图1）到3月14日，输入感染人数开始上升，此时上海本地感染数量仍然无幅度的改变，处于平稳状态。为了更仔细看清楚变化情况，我们将y轴的数量调整至0-1000例，这时候能清楚看到每天的变化，本地感染者（蓝色）数量从3月16日快速上升者。

```{r results='asis'}

sh_long %>% filter(检出阳性日期 > "2022-03-01") %>% filter(感染者类型 == "新增境外输入感染"|感染者类型 == "新增本地感染")  %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(alpha = 0.5) + 
  geom_line(alpha = 0.5) +
  scale_x_date(date_breaks = "5 day", date_labels = "%m-%d") +
  theme(legend.position = "none") +
  facet_zoom(ylim = c(0,1000), zoom.size = 1.5, show.area = T, split = F, shrink = T) +
  theme_bw()


```




## 重点关注3月的数据



```{r, eval=FALSE}
##设置网址
url_all <- paste0("https://wsjkw.sh.gov.cn/xwfb/index_",c(2:20),".html")
url_all <- c("https://wsjkw.sh.gov.cn/xwfb/index.html", url_all)
##自定义爬一级网页函数
webfun <- function(url){
pages <- read_html(url)
url2_sh <-  pages  %>%  
  html_nodes(".list-date a") %>% 
  html_attr("href") # 次级链接地址
title_sh <- pages %>% 
  html_nodes(".list-date a") %>% 
  html_attr("title") # 标题
date_sh <- pages %>% 
  html_node(".time") %>% 
  html_text() # 日期
data.frame( Date = date_sh,  Title = title_sh, Url = url2_sh) # 组合数据框
}
## 爬取1-20页内容
pages_1 <- map_df(url_all, webfun)
# 根据关键字段“居住信息” 筛选出需要的标题和链接
url_2 <-  pages_1 %>% dplyr::filter(str_detect(Title, "居住地信息")) %>% mutate(
  url_2 = paste0("https://wsjkw.sh.gov.cn", Url) ) %>% mutate(
    url_2 = if_else(str_detect(url_2, "weixin"), Url, url_2)
  ) %>% select(url_2) %>% unlist()

webfun2 <- function(url){

read_html(url) %>%
  html_nodes("span") %>% 
  html_text() %>% 
  .[str_detect(., "居住于")] %>%
  tibble(var_loca = .) %>% mutate(
    Date_rep = str_extract(var_loca, "[0-9]+月[0-9]+日") %>% paste0("2022年", .) %>% ymd() ,
    Local = str_extract(var_loca, "[\u4E00-\u9FA5]+区"),
    case_conf = str_extract(var_loca, "[0-9]+例本土") %>% str_extract(., "[0-9]+") %>% as.numeric(),
    case_none = str_extract(var_loca, "[0-9]+例本土无症状感染者") %>% str_extract(., "[0-9]+") %>% as.numeric(),
    case_sum = case_conf + case_none
  )
}
url_2 <- url_2[-6]

final_df <- map_df(url_2, webfun2)# 获取次级链接信息
final_df <- final_df %>%  dplyr::filter(!is.na(Date_rep))  # 筛选出需要的行

# 补充完整的区名
final_df <- final_df %>% mutate(
  case_sum = case_conf + case_none,
  Local = case_when(Local == "浦东新" ~ "浦东新区",
                    Local == "青浦新" ~ "青浦区",
                    Local == "奉贤无" ~ "奉贤区",
                    is.na(Local) ~ "奉贤区",
                    TRUE ~ Local)
)
final_df[is.na(final_df)] <- 0 # 替换缺失值为0
final_df <-  final_df %>% distinct() # 剔重
write.xlsx(final_df, ("data/shcovid.xlsx")) # 写出到EXCEL文件


```

```{r, warning=FALSE, include=TRUE, results='asis' }
final_df <- read.xlsx("data/shcovid.xlsx", detectDates = T  )
```

## 地区分布

获取了地区分布的疫情数据，结果很明显，浦东新区感染者数量最多，见下表：

```{r, results='asis', results='asis'}

final_df_wider <-  final_df %>% group_by(Date_rep, Local) %>% 
  summarise(case = sum(case_sum)) %>% arrange(-case) 
final_df_wider %>% pivot_wider(
  Local, names_from = Date_rep, values_from = case
) %>% knitr::kable()
```

画个玫瑰图看看，其实这个图并并能很好反应数据特征。

```{r}
windowsFonts(A = windowsFont("微软雅黑")) # 设置字体

final_df %>% ggplot(aes(x = factor(Date_rep), y = case_sum)) +
  geom_col(aes(fill = Local), width = 1.03) +
  coord_polar(theta = "x", start = 0, direction = 1) +
  theme(panel.background = element_blank(),
        panel.grid = element_blank(), 
        axis.title = element_blank(),
        legend.position = c(0.9,0.5),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        legend.title = element_blank(),
        legend.text = element_text(size = 5),
        text = element_text(family = "A")
        )+ geom_text(data = final_df %>% dplyr::filter(Local == "浦东新区"), aes(label = Date_rep ),
                                  nudge_y = 3000,
                                  size = 2, 
                                  ) +
  guides(fill = guide_legend(ncol = 2) ) +
  theme(plot.margin = unit(rep(0.5,4), "mm"),
        legend.key.size = unit(3,"mm"),
        aspect.ratio = 1) +
  scale_y_continuous(expand = c(0,0), limits = c(-500,25000)) 

```

感觉还不如单纯的条图清晰

```{r}
final_df %>% ggplot(aes(x = Date_rep, y = case_sum)) +
  geom_col(aes(fill = Local), col = "grey",width = 1.03) +
  #coord_polar(theta = "x", start = 0, direction = 1) +
  theme(panel.grid.minor =  element_blank(), 
        axis.title = element_blank(),
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 5)
        ) +
  guides(fill = guide_legend(ncol = 2) ) +
  theme(plot.margin = unit(rep(0.5,4), "mm"),
        legend.key.size = unit(3,"mm"),
        aspect.ratio = 1) +
  scale_y_continuous(expand = c(0,0)) +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d")
```



## 发现途径数据获取



```{r, eval=FALSE}
url_3 <-  pages_1 %>%
  dplyr::filter(str_detect(Title, "新增本土新冠肺炎确诊病例")) %>%
  mutate(
  url_3 = paste0("https://wsjkw.sh.gov.cn/", Url) ) %>% 
  select(url_3) %>%
  unlist()#组合成所需的所有次级链接地址
##建立函数
webfun3 <- function(url){
#pages <- read_html(url) %>% html_nodes("#ivs_content > p:nth-child(1)") %>% html_text()
pages <- read_html(url) %>% html_nodes("#ivs_content") %>% html_text()
tb_2 <- tibble(content = pages)
tb_2_sep <-  tb_2 %>% mutate(
  rep_date = str_extract(content, "[0-9]+年[0-9]+月[0-9]+日"),
  case_c = str_extract(content, "例[0-9]+") %>% str_extract("[0-9]+"),
  case_u = str_extract(content, "者[0-9]+") %>% str_extract("[0-9]+"),
  case_c_zh = str_extract(content, "中[0-9]+例确诊病例为") %>% str_extract("[0-9]+"),
  case_c_gk = str_extract(content, "[0-9]+例确诊病例和") %>% str_extract("[0-9]+"),
  case_u_gk = str_extract(content,"和[0-9]+例无症状感染者在隔离管控中发现" ) %>%  str_extract("[0-9]+")) %>%
  select(rep_date, case_c, case_u, case_c_zh, case_c_gk, case_u_gk
)
}
# 爬取数据
tb_fenlei <- map_df(url_3, webfun3)

# 剔重
tb_fenlei <-  tb_fenlei %>% filter(!is.na(case_c)) %>% distinct(case_u, .keep_all = T)
tb_fenlei <-  tb_fenlei %>% mutate(
  across(case_c:case_u_gk, as.numeric)
)

tb_fenlei[is.na(tb_fenlei)] <- 0 #替换缺失值
# 计算各类型人群
tb_fenlei <-  tb_fenlei %>% mutate(
  case_c_fx = case_c - case_c_zh,
  case_u_fx = case_u - case_u_gk,
  case_all = case_c + case_u - case_c_zh,
  case_all_gk = case_c_gk + case_u_gk,
  case_all_fx = case_all - case_all_gk,
  rep_date = as.Date.character(rep_date, "%Y年%m月%d日")
)
# 写出数据
write.xlsx(tb_fenlei, "data/case_fenlei.xlsx")

```


获取数据信息如下

```{r, results='asis'}
tb_fenlei_final <- read.xlsx("data/case_fenlei.xlsx", detectDates = T )
knitr::kable(head(tb_fenlei_final))


```

## 时间分布

首先看看的大体趋势，从3月13日起，随着时间感染者数量几乎是成指数增加。但是从风险人群（红线）中检出感染者数量似乎有下降的迹象。
```{r}
tb_fenlei_long <-  tb_fenlei_final %>%
  select(rep_date, case_all_fx, case_all_gk) %>%  
  pivot_longer(cols = c(case_all_fx, case_all_gk), names_to = "type", values_to = "cases") 



tb_fenlei_long %>% ggplot(aes(x = rep_date, y = cases)) +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d", expand = c(0.05,0.05)) +
  geom_line(aes(col = type)) +
  geom_point(aes(col = type)) +
  geom_smooth(aes(group = type), col = "grey", linetype = "dashed", alpha = 0.2, se = T) +
  theme(panel.grid.minor = element_blank())
    
  

```

## 人群分布

再看看每天感染者发现途径的变化情况。红色代表每天从风险人群中发现感染者，蓝色代表从管控人群。可见从风险人群中发现感染者的比例大体趋势是逐渐减少的。这种情况视乎是一个好的征兆。但同时需要考虑随着防控政策的变化，对两类人群的定义是否也随之变化？
```{r}
tb_fenlei_long %>% ggplot(aes(x = rep_date, y = cases)) +
  geom_col(aes(fill = type), position = "fill") +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d", expand = c(0.05,0.05))
```
再单独看看风险人群中检出感染者的情况，从了4月3日，4月4日连续两天下降，且看起来几乎是呈直线下降，这是一个好兆头。预示着，随着大规模核酸检测工作的全面铺开，从社会面发现感染者数量的增长势头将会慢慢遏制。

```{r}
tb_fenlei_long %>% filter(type == "case_all_fx") %>% 
  ggplot(aes(rep_date, cases)) +
  geom_point() +
  geom_line() +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d") +
  scale_y_continuous(breaks = seq(0,1500,200))+
  theme(panel.grid.minor = element_blank()) +
  geom_line(data = .%>% filter(rep_date > "2022-04-01"), col =4 , size = 1) +
  geom_point(data = .%>% filter(rep_date > "2022-04-01"), col = 4, size = 2) +
  geom_text(data = .%>% filter(rep_date > "2022-04-01"),aes(label = cases), nudge_y = 50 , col = 4)
```

# 简单总结

从上面的简单分析可得知，上海市境外输入感染者数量，从去年2月20日左右开始上升，增加的幅度从平均12例左右，到2月24左右达到85例左右。确实存在一个数量快速增加的阶段，之后过了大概一周的时间上海本地感染者开始快速上升，我们不得不猜测这二者之间可能存在某种联系。最后一张图添加了平滑曲线，做一个大致的预测。从图可见上海这一次疫情新增病人数量仍未到达所谓的“拐点”~~没爬错数据~~。随着检测人数的增加，防控措施的进一步加码和落实，相信这一波疫情会逐渐得到控制。

```{r, results='asis'}
covidsh <-  sh_long %>% filter(reportdate > as.Date.character("2022-3-1")) %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_line(size = 1) +
   geom_point( size = 0.5)+
  geom_smooth(aes(group = 感染者类型 ) , col = "grey", linetype = "dashed", alpha = 0.2, se = T) +
  scale_x_date(date_breaks = "2 day", date_labels = "%m/%d") +
  theme_bw() +
  theme(
        axis.text = element_text(size = 9),
        legend.title = element_blank(),
        legend.text = element_text(size = 9),
        panel.grid = element_blank(),
        legend.position = "top")
covidsh
```

## 预测

曲线拟合如下
```{r}
tb_fenlei_final

mod_data <-  tb_fenlei_final %>% select(rep_date, case_all,case_c, case_u) %>% arrange(rep_date) %>% 
   filter(rep_date > as.Date("2022-3-1")) %>%  mutate(days = 1 : nrow(.))


mod_all <- lm( case_all ~ days + I(days^2)+ I(days^3), data = mod_data)
mod_all2 <- lm(case_all ~ I(days^3), data = mod_data )

newdata <- data.frame(days =  1  : c(nrow(mod_data) + 3), Date = seq( as.Date("2022-3-18"), by = "day", length.out = nrow(mod_data) + 3 )  )
pre_data <- newdata %>%  mutate(
  pred_all = predict(object = mod_all2, newdata = newdata) %>% round(0)
) 

Rsq <- expression(R^2 == format(summary(mod_all2)$r.square, digits = 2) )
mod_data %>% ggplot() +
  geom_line(aes(rep_date, case_all,  col = "red"), linetype = 1, size = 0.5) +
  #geom_smooth(aes(rep_date, case_all), method = "gam") +
  geom_point(aes(rep_date, case_all), col = "red", size = 1.2) +
  geom_text(data = mod_data %>% filter(rep_date > as.Date("2022-4-7")), aes(rep_date, case_all, label = case_all), col = "red4", nudge_x = - 2, size =3) +
  geom_line(data = pre_data, aes(Date, pred_all,col = "green") , linetype = 2) +
  geom_point(data = pre_data, aes(Date, pred_all), col = "green", size = 1.5, shape = 21) +
  geom_text(data = pre_data %>% filter(Date >  as.Date("2022-4-7")) , 
            aes(Date, pred_all, label =  pred_all),col = "green4", nudge_x= 1, nudge_y = -500, size = 3) +  
  theme_bw() +
  scale_color_manual(name = "", values = c("red", "green"), breaks = c("red", "green"), labels = c("实际值", "预测值")) +
  theme(legend.position = "right") +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d") +
  xlab("检出日期") + ylab("感染者数") +
  scale_y_continuous(breaks = seq(0,30000, 5000)) +
  theme(panel.grid.minor = element_blank()) +
  annotate("text", x = as.Date("2022-4-10"), y = 100, parse = T, label = "R^2 == 0.97")
  

```


# 动力学微分方程

由于数字获取不准确，缺失数据太多，无法顺利拟合。 

```{r, echo=FALSE, eval=FALSE}
library(deSolve)
library(FME)
SEIRModel <-  function(Time, InitialStat, Parameter){
  with(as.list(c(InitialStat, Parameter)),{
    dS <- -c *  S
    dI <- c * S  - r * I
    dR <- r * I
    return(list(c(dS,  dI, dR)))
  })
}

initialstat <- c( S = 100000, #Proportion fo Susceptible population
                  I = 158, #Infection pop
                  R = 0 )# recovery pop.

params <- c(c = 0.09, # The rate of the E became I
            r = 0.03 # The recovery rate of the infection pop.
            )

times <- c(1:22)
##Solve the differential equation with "deSolve" package

out <- ode(y = initialstat, times = times, parms = params, func = SEIRModel)
#Basic plot
plot(out)
#Advanced plot


## 数据拟合
fitdata <- read.xlsx("data/fitdata.xlsx")

MyCost <- function(Pars){
  out2 <- ode(y = initialstat, func = SEIRModel, parms = Pars, times = fitdata$time)
  return(modCost(model = out2, obs = fitdata, x = "time"))
}
myfit <-  modFit(f = MyCost, p = params, method = "Marq", lower = c(0,0), upper = c(1,1))
ode(y = initialstat, times = 1:25, parms = myfit$par, func = SEIRModel)
summary(myfit)

rc <- 0.09/0.03
mydf <- gather(data.frame(out), key = "PopTypes", value = "Cases", S:R)
sirfig <- ggplot(mydf, aes(x = time, y = Cases)) + geom_line(aes(col = PopTypes), size = 1) +
  geom_text(aes(x = 22, y =0.8, label = paste("Rc =",rc))) + theme_bw() + xlab("Days") + ylab("Predicted Cases") +
  theme(legend.title = element_blank())
sirfig
```

# 时间序列模型

## 加载相应的包

```{r}
library(tidymodels)
library(modeltime)
library(timetk)
library(forecast)

```

```{r}
ts_data <- mod_data %>% select(rep_date, case_all)
ts_data <- ts_data %>% rbind(data.frame(rep_date = ymd("2022-4-9", "2022-4-10"), case_all = c(24752, 26760)))
ts_data %>% plot_time_series(rep_date, case_all, .interactive = F)
data_sp <- ts_data %>% time_series_split(assess = "1 week", cumulative = T)
data_sp %>% tk_time_series_cv_plan() %>% 
   plot_time_series_cv_plan (rep_date, case_all, .interactive = F)

model_arima  <-  arima_reg() %>% 
  set_engine("auto_arima") %>% 
  fit(case_all ~ rep_date, training(data_sp))
model_arima

model_prophet = prophet_reg(  ) %>% 
  set_engine("prophet") %>% 
  fit(case_all ~ rep_date, training(data_sp))

model_prophet


```

机器学习模型
```{r}
recipe_spec = recipe(case_all ~ rep_date, training(data_sp)) %>% 
  step_timeseries_signature(rep_date) %>% 
  step_rm(contains("am.pm"), contains("hour"), contains("minute"),
          contains("second"), contains("xts")) %>% 
  step_fourier(rep_date, period = 90, K =5) %>% 
  step_dummy(all_nominal())

recipe_spec %>% prep() %>% juice()

model_spec_glmnet <- linear_reg(penalty = 0.01, mixture = 0.5) %>%
  set_engine("glmnet")

workflow_fit_glmnet <- workflow() %>%
  add_model(model_spec_glmnet) %>%
  add_recipe(recipe_spec %>% step_rm(rep_date)) %>%
  fit(training(data_sp))


```
随机森林模型

```{r}
model_spec_rf <- rand_forest(trees = 500, min_n = 15) %>%
  set_engine("randomForest")

workflow_fit_rf <- workflow() %>%
  add_model(model_spec_rf) %>%
  add_recipe(recipe_spec %>% step_rm(rep_date)) %>%
  fit(training(data_sp))


```
propboost

```{r}
model_spec_prophet_boost <- prophet_boost(seasonality_yearly = TRUE) %>%
  set_engine("prophet_xgboost") 

workflow_fit_prophet_boost <- workflow() %>%
  add_model(model_spec_prophet_boost) %>%
  add_recipe(recipe_spec) %>%
  fit(training(data_sp))

```


校准模型
```{r}
model_table <- modeltime_table(
  model_arima, 
  model_prophet,
  workflow_fit_glmnet,
  workflow_fit_rf,
  workflow_fit_prophet_boost
) 

model_table

```
校准
```{r}
calibration_table = model_table %>% 
  modeltime_calibrate(testing(data_sp))

calibration_table
```

## 预测

```{r}

calibration_table %>%
  modeltime_forecast(actual_data = ts_data) %>%
  plot_modeltime_forecast(.interactive = FALSE)

```

## 准确率

从下表可见拟合度最高的前面1，2，5这3个模型。

```{r, results='asis'}
calibration_table %>%
  modeltime_accuracy() %>%
  table_modeltime_accuracy(.interactive = FALSE)
```



## 模型选择

重新拟后，感觉最好的机器学习的Glmnet模型

```{r}

pre_data_tim <-  calibration_table %>%
  # Remove ARIMA model with low accuracy
  filter(.model_id != 4) %>%
  # Refit and Forecast Forward
  modeltime_refit(ts_data) %>%
  modeltime_forecast(h = "1 weeks", actual_data = ts_data) 
pre_data_tim %>%
  plot_modeltime_forecast(.interactive = FALSE)
 
```

## 无责任预测

筛选1，2，3号模型的预测值如下:

```{r, results='asis'}
pre_data_tim %>% 
  filter(str_detect(.model_desc, "ARIMA|PROPHET|XGBOOST ERRORS"), .index < ymd("2022-04-14")) %>% 
  select(.model_desc, .index, .value) %>% rename(
    "模型" = ".model_desc",
    "日期" = ".index",
    "预测值" = ".value"
  ) %>% 
  knitr::kable()
```



