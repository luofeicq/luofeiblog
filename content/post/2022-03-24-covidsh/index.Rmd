---
title: R爬上海疫情数据简单可视化分析(4月6日更新)
author: Luo Fei
date: '2022-03-24'
slug: covidsh
categories:
  - 中文
tags:
  - Tec
  - R
output:
    blogdown::html_page:
        number_sections: true
math: true
showToc: true
TocOpen: false
toc-title: "目录"
draft: false
hidemeta: false
comments: false
description: "无责任猜测。"
canonicalURL: ""
disableShare: false
hideSummary: false
searchHidden: false
ShowReadingTime: false
ShowBreadCrumbs: true
ShowPostNavLinks: true
cover:
    image: "" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page    
---

```{r,setup,include=FALSE}
options(digits = 2)

##knitr设置
knitr::opts_chunk$set(
  prompt = FALSE,
  comment = "#",
  results = "markup",
  error = FALSE,
  message = FALSE,
  fig.align = "center",
  collapse = FALSE,
  tidy = F,
  echo = F,
  message = F,
  results = "hide",
  warning = F
  )

```
今天本地的疫情终于没有增加，抽了点时间关注其他地区的疫情形势。看官方通报的数据，对曾经的模范城市的疫情有兴趣，决定来简单看看。

# 数据获取

## 数据来源

要获取准确的数据，当然是上官方网站。打开上海市卫健委的官网(https://wsjkw.sh.gov.cn/xwfb/index.html)，疫情数据公告都在“新闻发布”栏目中，再仔细一看，疫情信息的标题中就包含了所有新增、确诊数据。真是太方便了。新闻页面也是连续的以`_[num]`为页码的编号，这种页面爬起来不要太省事。

## R包

首先加载需要用到的包，主要使用`rvest`包获取静态网页信息，谷歌浏览器`selectorgadget`插件获取需要信息的节点，`tidyverse`整理数据，绘图，`lubridate`包处理日期变量。

```{r, eval=TRUE, include=TRUE, warning=FALSE, echo=TRUE}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)
library(openxlsx)
library(ggforce)

```



## 主页信息获取

使用`selectorgadget`在网页上找到需要的变量“时间”和“标题”，对应的node分别是`.time`和`.list-data a`，使用`rvest`抓取后，转为文本存储在列表中。


```{r, eval=FALSE, echo=TRUE}
## 设置url
url <- "https://wsjkw.sh.gov.cn/xwfb/index.html"
## 获取首页信息
content <- read_html(url)
reportdate <- content %>%  html_nodes(".time") %>% html_text() %>% as.Date.character()
title <-  content %>% html_nodes(".list-date a") %>% html_text() 
basedata <- tibble(reportdate = reportdate, title = title)

```

## 爬取所有数据

上海的疫情变化主要从3月开始，但为了查看之前的变化，是否输入病例压力增大导致本次疫情，因此爬取了2到200页到的数据。改用`purrr`包的`map_df`函数爬取。

```{r, eval=FALSE, echo=TRUE}
##使用循环爬取2到200也网页信息
url_1 <- c("https://wsjkw.sh.gov.cn/xwfb/index.html",
           paste0("https://wsjkw.sh.gov.cn/xwfb/index_", c(2:200), ".html"))
webfun_0 <- function(url){
    webpage <- read_html(url) 
    tibble( "reportdate" = webpage %>% html_nodes(".time") %>% html_text(),
    "title" = webpage %>% html_nodes(".list-date a") %>% html_attr("title"))
  }
basedata <- map_df(url_1, webfun_0) 


## 写出数据
write.csv(basedata,"data/basedata.csv")

```

## 数据清洗

这一步比较麻烦的是对标题中日期字符的整理。使用`str_extract_all`命令后提取的日期，变成了列表。再合并为向量形势的日期格式数据时出了点麻烦。最后使用了笨办法`for`循环`unlist`后再`paste0`合并。其实直接用标题前的日期-1没什么大的误差，主要是在跟自己较劲搞得这么麻烦。

```{r, include=FALSE, message=FALSE}
basedata <- read.csv("data/basedata.csv")
## 筛选出包含疫情信息的标题
sh01 <-  basedata %>% filter(str_detect(title, "新冠肺炎确诊病例"))

sh02 <- sh01 %>% mutate(
  reportdate = as.Date.character(reportdate, "%Y-%m-%d"),
  title = if_else(str_detect(title, "本地"), str_replace(title, "本地", "本土"), title),
  检出阳性日期 = str_extract(title, "[0-9]+年[0-9]+月[0-9]+日") %>% as.Date.character("%Y年%m月%d日"),
  新增确诊 = str_extract(title, "新增本土新冠肺炎确诊病例[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  新增无症状 = str_extract(title, "无症状感染者[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  新增境外输入确诊 = str_extract(title, "境外.+[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  新增境外无症状输入 = str_extract(title, "境外输入性无症状感染者+[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  治愈数 = str_extract(title, "治愈出院[0-9]+") %>% str_extract("[0-9]+") %>% as.numeric(),
  解除 = str_extract(title, "解除.+") %>% str_extract("[0-9]+") %>% as.numeric())



sh02 <- sh02 %>% mutate(
  检出阳性日期 = if_else(is.na(检出阳性日期), reportdate-1, 检出阳性日期)
)
sh02[is.na(sh02)] <- 0
sh02 <- sh02 %>% mutate(
  新增本地感染 = 新增确诊 + 新增无症状,
  新增境外输入感染 = 新增境外输入确诊 + 新增境外无症状输入
 )
## 转换为长数据

sh_long <-  sh02 %>% pivot_longer(cols =  新增确诊:新增境外输入感染, names_to = "感染者类型", values_to = "病例数" )



```

# 好了，开始分析吧

## 先画个简单的图看看大趋势

以报告感染者类型为颜色看，从20年1月到21年12月期间，上海的感染人数几乎在处于一个长期稳定的状态，从2022年3月开始呈直升飞机式的增长。奇怪的是中间咋有个空白区，没有数据。查看原始网页，发现网站从2021年11月6日-2022年1月1日没有更新数据。这是一个奇怪的现象。不过没关系，这不影响我们后面的分析，~~这对原因分析有很大关系~~。

```{r, results='asis'}

sh_long %>% ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(alpha = 0.3, position = "jitter") + 
  scale_x_date(date_breaks = "3 month", date_labels = "%y-%m")
```

##  输入感染者的趋势


对上海这波疫情有个合理的猜测是，1月初上海接纳大量某地的航班的，导致上海市输入疫情压力陡增，再加上Omicron变异株超强传播能力，双重压力下导致这个模范城市失守。好吧，我们来看看是否能验证输入压力陡增这个猜测。从图上看新增输入的感染者数量变化并不大，鉴于确诊和无症状都属与感染这，下一步我们把本地和输入的感染者的合计数的变化可视化看看。

```{r, results='asis'}

sh_long %>% filter( 感染者类型 == "新增境外输入感染"  ) %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(position = "jitter", alpha = 0.2) + 
  scale_x_date(date_breaks = "3 month", date_labels = "%y-%m")

```

## 感染者总数

先用二者做个散点图看看,如图，完全看不出啥关系啊。。。。输入感染者较多的时候，反而本地感染处于低水平。这个图像，线性回归暂时也不考虑。我们还是从感染者数量和时间的关系看看。



```{r, results='asis'}
sh_long %>% filter(检出阳性日期 > as.Date("2022-2-11")) %>% 
  filter(感染者类型 == "新增境外输入感染"|感染者类型 == "新增本地感染") %>% 
  select(检出阳性日期, 感染者类型, 病例数) %>% pivot_wider(names_from = 感染者类型, values_from = 病例数) %>% 
  ggplot() +
  geom_point(aes(x = 新增本地感染, y = 新增境外输入感染))
```

画出来如下，大致能看出在3月14日前，上海几乎没有本土感染病例。
```{r, results='asis'}
sh_long %>% filter(感染者类型 == "新增境外输入感染"|感染者类型 == "新增本地感染")  %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(alpha = 0.5) + 
  geom_line(alpha = 0.5) +
  scale_x_date(date_breaks = "3 month", date_labels = "%y-%m") 

```

## 2022年1到三月


我们把时间尺度拉大到2月中旬到3月看看。放大后（下图1）到3月14日，输入感染人数开始上升，此时上海本地感染数量仍然无幅度的改变，处于平稳状态。为了更仔细看清楚变化情况，我们将y轴的数量调整至0-1000例，这时候能清楚看到每天的变化，本地感染者（蓝色）数量从3月16日快速上升者。

```{r results='asis'}

sh_long %>% filter(检出阳性日期 > "2022-01-01") %>% filter(感染者类型 == "新增境外输入感染"|感染者类型 == "新增本地感染")  %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_point(alpha = 0.5) + 
  geom_line(alpha = 0.5) +
  scale_x_date(date_breaks = "5 day", date_labels = "%m-%d") +
  theme(legend.position = "none") +
  facet_zoom(ylim = c(0,1000), zoom.size = 1.5, show.area = T, split = F, shrink = T) +
  theme_bw()


```




## 重点关注3月的数据



```{r, eval=FALSE}
##设置网址
url_all <- paste0("https://wsjkw.sh.gov.cn/xwfb/index_",c(2:20),".html")
url_all <- c("https://wsjkw.sh.gov.cn/xwfb/index.html", url_all)
##自定义爬一级网页函数
webfun <- function(url){
pages <- read_html(url)
url2_sh <-  pages  %>%  
  html_nodes(".list-date a") %>% 
  html_attr("href") # 次级链接地址
title_sh <- pages %>% 
  html_nodes(".list-date a") %>% 
  html_attr("title") # 标题
date_sh <- pages %>% 
  html_node(".time") %>% 
  html_text() # 日期
data.frame( Date = date_sh,  Title = title_sh, Url = url2_sh) # 组合数据框
}
## 爬取1-20页内容
pages_1 <- map_df(url_all, webfun)
# 根据关键字段“居住信息” 筛选出需要的标题和链接
url_2 <-  pages_1 %>% dplyr::filter(str_detect(Title, "居住地信息")) %>% mutate(
  url_2 = paste0("https://wsjkw.sh.gov.cn/", Url) ) %>% select(url_2) %>% unlist()#组合成所需的所有次级链接地址
url_2[1] <- "https://mp.weixin.qq.com/s/djwW3S9FUYBE2L5Hj94a3A"
# 设置爬取次级网页关键内容的函数
webfun2 <- function(url){
test <-  read_html(url) %>% html_nodes("p") %>% html_text() %>% tibble()#获取次级网页所有文本信息
colnames(test) <- "Text" # 设置列名
test %>% 
  dplyr::filter(str_detect(Text, "分别居住")) %>% # 筛选出含有大区病例信息的段落
  separate(Text, c("Date", "col1", "col2"), sep = "，|、") %>% # 根据逗号和分号拆分列
  mutate(
    Date_rep = as.Date.character(Date, "%Y年%m月%d日"),# 转换日期类型
    Local = str_extract(col1, "..."), # 提取3字符的地区名
    case_conf = str_extract(col1, "[0-9]+") %>% as.numeric(), #提取确诊病例数
    case_none = str_extract(col2, "[0-9]+") %>% as.numeric() # 提取无症状数
  ) %>% select(Date_rep, Local, case_conf, case_none) # 列重新排序
}

final_df <- map_df(url_2, webfun2)# 获取次级链接信息
final_df <- final_df %>%  dplyr::filter(!is.na(Date_rep))  # 筛选出需要的行
final_df[is.na(final_df)] <- 0 # 替换缺失值为0
# 补充完整的区名
final_df <- final_df %>% mutate(
  case_sum = case_conf + case_none,
  Local = case_when(Local == "浦东新" ~ "浦东新区",
                    Local == "青浦新" ~ "青浦区",
                    Local == "奉贤无" ~ "奉贤区",
                    TRUE ~ Local)
)
final_df <-  final_df %>% distinct() # 剔重
write.xlsx(final_df, ("data/shcovid.xlsx")) # 写出到EXCEL文件
```

```{r, warning=FALSE, include=TRUE, results='asis' }
final_df <- read.xlsx("data/shcovid.xlsx", detectDates = T  )
```

## 地区分布

重新爬了一下按地区分布的，结果很明显，浦东新区感染者数量最多，见下表：

```{r, results='asis', results='asis'}

final_df_wider <-  final_df %>% group_by(Date_rep, Local) %>% 
  summarise(case = sum(case_sum)) %>% arrange(-case) 
final_df_wider %>% pivot_wider(
  Local, names_from = Date_rep, values_from = case
) %>% knitr::kable()
```

画个玫瑰图看看，其实这个图并并能很好反应数据特征。

```{r}
windowsFonts(A = windowsFont("微软雅黑")) # 设置字体

final_df %>% ggplot(aes(x = factor(Date_rep), y = case_sum)) +
  geom_col(aes(fill = Local), width = 1.03) +
  coord_polar(theta = "x", start = 0, direction = 1) +
  theme(panel.background = element_blank(),
        panel.grid = element_blank(), 
        axis.title = element_blank(),
        legend.position = c(0.9,0.5),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        legend.title = element_blank(),
        legend.text = element_text(size = 5),
        text = element_text(family = "A")
        )+ geom_text(data = final_df %>% dplyr::filter(Local == "浦东新区"), aes(label = Date_rep ),
                                  nudge_y = 3000,
                                  size = 2, 
                                  ) +
  guides(fill = guide_legend(ncol = 2) ) +
  theme(plot.margin = unit(rep(0.5,4), "mm"),
        legend.key.size = unit(3,"mm"),
        aspect.ratio = 1) +
  scale_y_continuous(expand = c(0,0), limits = c(-500,15000)) 

```

感觉还不如单纯的条图清晰

```{r}
final_df %>% ggplot(aes(x = Date_rep, y = case_sum)) +
  geom_col(aes(fill = Local), col = "grey",width = 1.03) +
  #coord_polar(theta = "x", start = 0, direction = 1) +
  theme(panel.grid.minor =  element_blank(), 
        axis.title = element_blank(),
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 5)
        ) +
  guides(fill = guide_legend(ncol = 2) ) +
  theme(plot.margin = unit(rep(0.5,4), "mm"),
        legend.key.size = unit(3,"mm"),
        aspect.ratio = 1) +
  scale_y_continuous(expand = c(0,0)) +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d")
```



## 发现途径数据获取



```{r, eval=FALSE}
url_3 <-  pages_1 %>%
  dplyr::filter(str_detect(Title, "新增本土新冠肺炎确诊病例")) %>%
  mutate(
  url_3 = paste0("https://wsjkw.sh.gov.cn/", Url) ) %>% 
  select(url_3) %>%
  unlist()#组合成所需的所有次级链接地址
##建立函数
webfun3 <- function(url){
#pages <- read_html(url) %>% html_nodes("#ivs_content > p:nth-child(1)") %>% html_text()
pages <- read_html(url) %>% html_nodes("#ivs_content") %>% html_text()
tb_2 <- tibble(content = pages)
tb_2_sep <-  tb_2 %>% mutate(
  rep_date = str_extract(content, "[0-9]+年[0-9]+月[0-9]+日"),
  case_c = str_extract(content, "例[0-9]+") %>% str_extract("[0-9]+"),
  case_u = str_extract(content, "者[0-9]+") %>% str_extract("[0-9]+"),
  case_c_zh = str_extract(content, "中[0-9]+例确诊病例为") %>% str_extract("[0-9]+"),
  case_c_gk = str_extract(content, "[0-9]+例确诊病例和") %>% str_extract("[0-9]+"),
  case_u_gk = str_extract(content,"和[0-9]+例无症状感染者在隔离管控中发现" ) %>%  str_extract("[0-9]+")) %>%
  select(rep_date, case_c, case_u, case_c_zh, case_c_gk, case_u_gk
)
}
# 爬取数据
tb_fenlei <- map_df(url_3, webfun3)

# 剔重
tb_fenlei <-  tb_fenlei %>% filter(!is.na(case_c)) %>% distinct(case_u, .keep_all = T)
tb_fenlei <-  tb_fenlei %>% mutate(
  across(case_c:case_u_gk, as.numeric)
)

tb_fenlei[is.na(tb_fenlei)] <- 0 #替换缺失值
# 计算各类型人群
tb_fenlei <-  tb_fenlei %>% mutate(
  case_c_fx = case_c - case_c_zh,
  case_u_fx = case_u - case_u_gk,
  case_all = case_c + case_u,
  case_all_gk = case_c_gk + case_u_gk,
  case_all_fx = case_all - case_all_gk,
  rep_date = as.Date.character(rep_date, "%Y年%m月%d日")
)
# 写出数据
write.xlsx(tb_fenlei, "data/case_fenlei.xlsx")

```


获取数据信息如下

```{r, results='asis'}
tb_fenlei_final <- read.xlsx("data/case_fenlei.xlsx", detectDates = T )
knitr::kable(head(tb_fenlei_final))


```

## 时间分布

首先看看的大体趋势，从3月13日起，随着时间感染者数量几乎是成指数增加。但是从风险人群（红线）中检出感染者数量似乎有下降的迹象。
```{r}
tb_fenlei_long <-  tb_fenlei_final %>%
  select(rep_date, case_all_fx, case_all_gk) %>%  
  pivot_longer(cols = c(case_all_fx, case_all_gk), names_to = "type", values_to = "cases") 



tb_fenlei_long %>% ggplot(aes(x = rep_date, y = cases)) +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d", expand = c(0.05,0.05)) +
  geom_line(aes(col = type)) +
  geom_point(aes(col = type)) +
  geom_smooth(aes(group = type), col = "grey", linetype = "dashed", alpha = 0.2, se = T) +
  theme(panel.grid.minor = element_blank())
    
  

```

## 人群分布

再看看每天感染者发现途径的变化情况。红色代表每天从风险人群中发现感染者，蓝色代表从管控人群。可见从风险人群中发现感染者的比例大体趋势是逐渐减少的。这种情况视乎是一个好的征兆。但同时需要考虑随着防控政策的变化，对两类人群的定义是否也随之变化？
```{r}
tb_fenlei_long %>% ggplot(aes(x = rep_date, y = cases)) +
  geom_col(aes(fill = type), position = "fill") +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d", expand = c(0.05,0.05))
```
再单独看看风险人群中检出感染者的情况，从了4月3日，4月4日连续两天下降，且看起来几乎是呈直线下降，这是一个好兆头。预示着，随着大规模核酸检测工作的全面铺开，从社会面发现感染者数量的增长势头将会慢慢遏制。

```{r}
tb_fenlei_long %>% filter(type == "case_all_fx") %>% 
  ggplot(aes(rep_date, cases)) +
  geom_point() +
  geom_line() +
  scale_x_date(date_breaks = "2 day", date_labels = "%m-%d") +
  scale_y_continuous(breaks = seq(0,1500,200))+
  theme(panel.grid.minor = element_blank()) +
  geom_line(data = .%>% filter(rep_date > "2022-04-01"), col =4 , size = 1) +
  geom_point(data = .%>% filter(rep_date > "2022-04-01"), col = 4, size = 2) +
  geom_text(data = .%>% filter(rep_date > "2022-04-01"),aes(label = cases), nudge_y = 50 , col = 4)
```

# 简单总结

从上面的简单分析可得知，上海市境外输入感染者数量，从去年2月20日左右开始上升，增加的幅度从平均12例左右，到2月24左右达到85例左右。确实存在一个数量快速增加的阶段，之后过了大概一周的时间上海本地感染者开始快速上升，我们不得不猜测这二者之间可能存在某种联系。最后一张图添加了平滑曲线，做一个大致的预测。从图可见上海这一次疫情新增病人数量仍未到达所谓的“拐点”~~没爬错数据~~。随着检测人数的增加，防控措施的进一步加码和落实，相信这一波疫情会逐渐得到控制。

```{r, results='asis'}
covidsh <-  sh_long %>% filter(reportdate > as.Date.character("2022-3-1")) %>% 
  ggplot(aes(x = 检出阳性日期, y = 病例数, col = 感染者类型)) +
  geom_line(size = 1) +
   geom_point( size = 0.5)+
  geom_smooth(aes(group = 感染者类型 ) , col = "grey", linetype = "dashed", alpha = 0.2, se = T) +
  scale_x_date(date_breaks = "2 day", date_labels = "%m/%d") +
  theme_bw() +
  theme(
        axis.text = element_text(size = 9),
        legend.title = element_blank(),
        legend.text = element_text(size = 9),
        panel.grid = element_blank(),
        legend.position = "top")
covidsh
```
## 预测
建立线性回归模型, 4月7日，预测16912
```{r}
mod_data <-  tb_fenlei_final %>% select(case_all) %>% arrange(case_all) %>% 
  mutate(days = 1:22)
mod1 <- lm( case_all ~ days + I(days^2 + I(days^3)),data = mod_data)
summary(mod1)
newdata <- data.frame(days = 1:30)
predict(mod1)
newdata <- newdata %>% mutate(
  pred = predict(mod1, newdata)
)

mod_data %>% ggplot() +
  geom_line(aes(days, case_all))+
  geom_line(data = newdata, aes(days, pred), col = 3)

```






